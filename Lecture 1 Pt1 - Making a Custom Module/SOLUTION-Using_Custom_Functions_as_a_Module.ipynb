{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVUfM_N_VpQn"
   },
   "source": [
    "# Using Custom Functions as a Module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrtuCmjkVe9D"
   },
   "source": [
    "## Lesson Objectives\n",
    "- Move custom functions to .py file\n",
    "- Add the .py file to your PYTHONPATH (so it can be accessed anywhere on your PC)\n",
    "- Importing your custom module so that it auto-reloads when edited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t35FfIPlWIxU"
   },
   "source": [
    "## Modules Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olTCjiCLWKeZ"
   },
   "source": [
    "A module is simply a python file (.py) that contains functions and classes to be imported and used in Python notebooks/files. e.g. `import pandas as pd`\n",
    "\n",
    "A .py file that will be used as a module should:\n",
    "- Include the imports for every package used in the .py file\n",
    "- Only include static code (do not run the functions, just define them)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUdqVYmkaUXo"
   },
   "source": [
    "# How to Save Python Functions as Modules \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPWPulZFa5Jd"
   },
   "source": [
    "\n",
    "### Importing Custom Modules Locally\n",
    "\n",
    ">  ***What is the simplest way to save python functions so that they can be reused almost anywhere, without actually publishing a python package?***\n",
    "\n",
    "The simplest way to save Python functions so that they can be reused almost anywhere is to put them into a .py file (often referred to as a Python module) and import this file when you need the functions.\n",
    "\n",
    "Here's a step-by-step guide:\n",
    "\n",
    "1. **Create a Python module**. Let's say you have a function to add two numbers:\n",
    "\n",
    "   ```python\n",
    "   def add_two_numbers(a, b):\n",
    "       return a + b\n",
    "   ```\n",
    "\n",
    "   Save this function in a .py file. For example, save it as `my_functions.py`.\n",
    "\n",
    "2. **Use the function in another script**. You can import this module into another Python script like this:\n",
    "\n",
    "   ```python\n",
    "   import my_functions as mf\n",
    "   \n",
    "   result = mf.add_two_numbers(1, 2)\n",
    "   print(result)  # Output: 3\n",
    "   ```\n",
    "\n",
    "   Note: The Python script and `my_functions.py` should be in the same directory for this to work. If they're not, you'd need to adjust the system's PYTHONPATH to include the directory where your module resides.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrvv8Icja6vx"
   },
   "source": [
    "\n",
    "### Update Custom Functions while using in a notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFMyq54Ma_sN"
   },
   "source": [
    "\n",
    "> ***How can I import my custom my_functions module in a Jupyter Notebook in a way that lets me continue to edit the .py file and have the module automatically reload itself with the updated code?***\n",
    "\n",
    "If you're actively developing and changing a Python module, and you want those changes to be immediately available in your Jupyter Notebook, you can use the `autoreload` extension from IPython.\n",
    "\n",
    "Here's a step-by-step guide:\n",
    "\n",
    "1. **Load the `autoreload` extension**. In your Jupyter notebook, run the following code:\n",
    "\n",
    "   ```python\n",
    "   %load_ext autoreload\n",
    "   ```\n",
    "\n",
    "2. **Set autoreload mode**.\n",
    "   - `%autoreload 2`: Reloads all modules  every time before executing the Python code typed.\n",
    "\n",
    "\n",
    "   ```python\n",
    "   %autoreload 2\n",
    "   ```\n",
    "\n",
    "3. **Import your module**. Now, you can import your module:\n",
    "\n",
    "   ```python\n",
    "   import my_functions as mf\n",
    "   ```\n",
    "\n",
    "**Combined, the code would would look like the following (note that the extension is loaded *before* the import):**\n",
    "\n",
    "  ```python\n",
    "  %load_ext autoreload\n",
    "  %autoreload 2\n",
    "  import my_functions as mf\n",
    "  ```\n",
    "\n",
    "With this setup, changes made to the `my_functions.py` file will automatically be reflected in the Jupyter Notebook, without needing to restart the kernel or manually reimport the module. Note that this may not always work for certain types of changes, like changes to a class definition. It works best for function definitions.\n",
    "\n",
    "\n",
    "One important thing to note is that while `autoreload` can be very useful during development, it might make your code behave in ways you don't expect, since the module state can change without you explicitly reloading it. Therefore, once your code is stable, it is a good practice to remove the `autoreload` statements and import your modules in the standard way.\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NafiIgJVbA0T"
   },
   "source": [
    "\n",
    "### Common Issues & Gotcha's\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zayeMB1zbBvb"
   },
   "source": [
    "\n",
    "> ***What tips or things to watch out for do you have for someone who is moving functions from their jupyter notebook to a .py file for the very first time? What are some of the common gotcha's/issues that people run into?***\n",
    "\n",
    "Moving Python functions from a Jupyter Notebook to a .py file is generally a straightforward process, but there are some things you need to be mindful of. Here are some tips and common issues to watch out for:\n",
    "\n",
    "1. **Hardcoded Paths**: If you have hardcoded paths for file I/O operations, these may no longer be correct when you move your function to a .py file in a different directory. Consider using relative paths or dynamically generating paths using Python's `os` module.\n",
    "2. **Variable Dependencies**: In a Jupyter notebook, it's common to use globally defined variables within functions. If your function depends on global variables, ensure these are passed as arguments to the function in the .py file. Your functions should ideally not rely on global state.\n",
    "3. **Order of Execution**: In Jupyter, you can run cells out of order. This can sometimes create dependencies that aren't immediately obvious. When you move your code to a .py file, everything is run sequentially from top to bottom, so you may need to rearrange your function definitions.\n",
    "4. **Interactive Features**: Functions in Jupyter notebooks may leverage interactive features, such as magic commands (e.g., `%matplotlib inline`) or use `input()` to get user input. These may not work as expected in a .py file, and alternatives should be considered.\n",
    "5. **Error Handling**: In a Jupyter Notebook, you may have become accustomed to writing quick-and-dirty code without much error handling, since you can interactively debug issues. In a .py file, you should add appropriate error handling to your functions.\n",
    "6. **Imports**: Ensure that all necessary imports are included at the beginning of your .py file. You may have imported libraries in different cells of your notebook. It's important to consolidate these imports in your .py file.\n",
    "7. **Refactoring**: This is a good opportunity to refactor your code. Make sure each function does one thing and does it well. Ensure your function names and variables are descriptive, and consider adding docstrings to your functions to explain what they do.\n",
    "8. **Testing**: Make sure to test your functions after moving them to the .py file. Just because they worked in the notebook doesn't mean they'll still work in the new context.\n",
    "\n",
    "Remember, code in a .py file should be self-contained and not depend on any hidden state, which is a common gotcha when moving from Jupyter notebooks to .py files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üïπÔ∏èCodeAlong Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a custom_functions.py file in the top-level of this repository.\n",
    "- Move the evaluation functions below into the .py file\n",
    "- Import the custom_functions into this notebook.\n",
    "- Test evaluation fucntion with the provided joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9K6mGRzZdGB"
   },
   "source": [
    "## üößFunctions (To Move to .py file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classification_metrics(y_true, y_pred, label='',\n",
    "                           output_dict=False, figsize=(8,4),\n",
    "                           normalize='true', cmap='Blues',\n",
    "                           colorbar=False,values_format=\".2f\"):\n",
    "    \"\"\"Modified version of classification metrics function from Intro to Machine Learning.\n",
    "    Updates:\n",
    "    - Reversed raw counts confusion matrix cmap  (so darker==more).\n",
    "    - Added arg for normalized confusion matrix values_format\n",
    "    \"\"\"\n",
    "    # Get the classification report\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    \n",
    "    ## Print header and report\n",
    "    header = \"-\"*70\n",
    "    print(header, f\" Classification Metrics: {label}\", header, sep='\\n')\n",
    "    print(report)\n",
    "    \n",
    "    ## CONFUSION MATRICES SUBPLOTS\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=figsize)\n",
    "    \n",
    "    # Create a confusion matrix  of raw counts (left subplot)\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=None, \n",
    "                                            cmap='gist_gray_r',# Updated cmap\n",
    "                                            values_format=\"d\", \n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[0]);\n",
    "    axes[0].set_title(\"Raw Counts\")\n",
    "    \n",
    "    # Create a confusion matrix with the data with normalize argument \n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=normalize,\n",
    "                                            cmap=cmap, \n",
    "                                            values_format=values_format, #New arg\n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[1]);\n",
    "    axes[1].set_title(\"Normalized Confusion Matrix\")\n",
    "    \n",
    "    # Adjust layout and show figure\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return dictionary of classification_report\n",
    "    if output_dict==True:\n",
    "        report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "        return report_dict\n",
    "\n",
    "def evaluate_classification(model, X_train, y_train, X_test, y_test,\n",
    "                         figsize=(6,4), normalize='true', output_dict = False,\n",
    "                            cmap_train='Blues', cmap_test=\"Reds\",colorbar=False):\n",
    "  # Get predictions for training data\n",
    "  y_train_pred = model.predict(X_train)\n",
    "  # Call the helper function to obtain regression metrics for training data\n",
    "  results_train = classification_metrics(y_train, y_train_pred, #verbose = verbose,\n",
    "                                     output_dict=True, figsize=figsize,\n",
    "                                         colorbar=colorbar, cmap=cmap_train,\n",
    "                                     label='Training Data')\n",
    "  print()\n",
    "  # Get predictions for test data\n",
    "  y_test_pred = model.predict(X_test)\n",
    "  # Call the helper function to obtain regression metrics for test data\n",
    "  results_test = classification_metrics(y_test, y_test_pred, #verbose = verbose,\n",
    "                                  output_dict=True,figsize=figsize,\n",
    "                                         colorbar=colorbar, cmap=cmap_test,\n",
    "                                    label='Test Data' )\n",
    "  if output_dict == True:\n",
    "    # Store results in a dataframe if ouput_frame is True\n",
    "    results_dict = {'train':results_train,\n",
    "                    'test': results_test}\n",
    "    return results_dict\n",
    "\n",
    "      \n",
    "\n",
    "def evaluate_classification_network(model, \n",
    "                                    X_train=None, y_train=None, \n",
    "                                    X_test=None, y_test=None,\n",
    "                                    history=None, history_figsize=(6,6),\n",
    "                                    figsize=(6,4), normalize='true',\n",
    "                                    output_dict = False,\n",
    "                                    cmap_train='Blues',\n",
    "                                    cmap_test=\"Reds\",\n",
    "                                    values_format=\".2f\", \n",
    "                                    colorbar=False):\n",
    "    \"\"\"Evaluates a neural network classification task using either\n",
    "    separate X and y arrays or a tensorflow Dataset\n",
    "    \n",
    "    Data Args:\n",
    "        X_train (array, or Dataset)\n",
    "        y_train (array, or None if using a Dataset\n",
    "        X_test (array, or Dataset)\n",
    "        y_test (array, or None if using a Dataset)\n",
    "        history (history object)\n",
    "        \"\"\"\n",
    "    # Plot history, if provided\n",
    "    if history is not None:\n",
    "        plot_history(history, figsize=history_figsize)\n",
    "    ## Adding a Print Header\n",
    "    print(\"\\n\"+'='*80)\n",
    "    print('- Evaluating Network...')\n",
    "    print('='*80)\n",
    "    ## TRAINING DATA EVALUATION\n",
    "    # check if X_train was provided\n",
    "    if X_train is not None:\n",
    "        ## Check if X_train is a dataset\n",
    "        if hasattr(X_train,'map'):\n",
    "            # If it IS a Datset:\n",
    "            # extract y_train and y_train_pred with helper function\n",
    "            y_train, y_train_pred = get_true_pred_labels(model, X_train)\n",
    "        else:\n",
    "            # Get predictions for training data\n",
    "            y_train_pred = model.predict(X_train)\n",
    "        ## Pass both y-vars through helper compatibility function\n",
    "        y_train = convert_y_to_sklearn_classes(y_train)\n",
    "        y_train_pred = convert_y_to_sklearn_classes(y_train_pred)\n",
    "        \n",
    "        # Call the helper function to obtain regression metrics for training data\n",
    "        results_train = classification_metrics(y_train, y_train_pred, \n",
    "                                         output_dict=True, figsize=figsize,\n",
    "                                             colorbar=colorbar, cmap=cmap_train,\n",
    "                                               values_format=values_format,\n",
    "                                         label='Training Data')\n",
    "        \n",
    "        ## Run model.evaluate         \n",
    "        print(\"\\n- Evaluating Training Data:\")\n",
    "        print(model.evaluate(X_train, return_dict=True))\n",
    "    \n",
    "    # If no X_train, then save empty list for results_train\n",
    "    else:\n",
    "        results_train = []\n",
    "    ## TEST DATA EVALUATION\n",
    "    # check if X_test was provided\n",
    "    if X_test is not None:\n",
    "        ## Check if X_train is a dataset\n",
    "        if hasattr(X_test,'map'):\n",
    "            # If it IS a Datset:\n",
    "            # extract y_train and y_train_pred with helper function\n",
    "            y_test, y_test_pred = get_true_pred_labels(model, X_test)\n",
    "        else:\n",
    "            # Get predictions for training data\n",
    "            y_test_pred = model.predict(X_test)\n",
    "        ## Pass both y-vars through helper compatibility function\n",
    "        y_test = convert_y_to_sklearn_classes(y_test)\n",
    "        y_test_pred = convert_y_to_sklearn_classes(y_test_pred)\n",
    "        \n",
    "        # Call the helper function to obtain regression metrics for training data\n",
    "        results_test = classification_metrics(y_test, y_test_pred, \n",
    "                                         output_dict=True, figsize=figsize,\n",
    "                                             colorbar=colorbar, cmap=cmap_test,\n",
    "                                              values_format=values_format,\n",
    "                                         label='Test Data')\n",
    "        \n",
    "        ## Run model.evaluate         \n",
    "        print(\"\\n- Evaluating Test Data:\")\n",
    "        print(model.evaluate(X_test, return_dict=True))\n",
    "      \n",
    "    # If no X_test, then save empty list for results_test\n",
    "    else:\n",
    "        results_test = []\n",
    "      \n",
    "    # Store results in a dictionary\n",
    "    results_dict = {'train':results_train,\n",
    "                    'test': results_test}\n",
    "    if output_dict == True:\n",
    "        return results_dict\n",
    "\n",
    "def plot_history(history,figsize=(6,8)):\n",
    "    # Get a unique list of metrics \n",
    "    all_metrics = np.unique([k.replace('val_','') for k in history.history.keys()])\n",
    "    # Plot each metric\n",
    "    n_plots = len(all_metrics)\n",
    "    fig, axes = plt.subplots(nrows=n_plots, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    # Loop through metric names add get an index for the axes\n",
    "    for i, metric in enumerate(all_metrics):\n",
    "        # Get the epochs and metric values\n",
    "        epochs = history.epoch\n",
    "        score = history.history[metric]\n",
    "        # Plot the training results\n",
    "        axes[i].plot(epochs, score, label=metric, marker='.')\n",
    "        # Plot val results (if they exist)\n",
    "        try:\n",
    "            val_score = history.history[f\"val_{metric}\"]\n",
    "            axes[i].plot(epochs, val_score, label=f\"val_{metric}\",marker='.')\n",
    "        except:\n",
    "            pass\n",
    "        finally:\n",
    "            axes[i].legend()\n",
    "            axes[i].set(title=metric, xlabel=\"Epoch\",ylabel=metric)\n",
    "    # Adjust subplots and show\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def convert_y_to_sklearn_classes(y, verbose=False):\n",
    "    # If already one-dimension\n",
    "    if np.ndim(y)==1:\n",
    "        if verbose:\n",
    "            print(\"- y is 1D, using it as-is.\")\n",
    "        return y\n",
    "        \n",
    "    # If 2 dimensions with more than 1 column:\n",
    "    elif y.shape[1]>1:\n",
    "        if verbose:\n",
    "            print(\"- y is 2D with >1 column. Using argmax for metrics.\")   \n",
    "        return np.argmax(y, axis=1)\n",
    "    \n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"y is 2D with 1 column. Using round for metrics.\")\n",
    "        return np.round(y).flatten().astype(int)\n",
    "\n",
    "\n",
    "def get_true_pred_labels(model,ds):\n",
    "    \"\"\"Gets the labels and predicted probabilities from a Tensorflow model and Dataset object.\n",
    "    Adapted from source: https://stackoverflow.com/questions/66386561/keras-classification-report-accuracy-is-different-between-model-predict-accurac\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred_probs = []\n",
    "    \n",
    "    # Loop through the dataset as a numpy iterator\n",
    "    for images, labels in ds.as_numpy_iterator():\n",
    "        \n",
    "        # Get prediction with batch_size=1\n",
    "        y_probs = model.predict(images, batch_size=1, verbose=0)\n",
    "        # Combine previous labels/preds with new labels/preds\n",
    "        y_true.extend(labels)\n",
    "        y_pred_probs.extend(y_probs)\n",
    "    ## Convert the lists to arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_probs = np.array(y_pred_probs)\n",
    "    \n",
    "    return y_true, y_pred_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text_vectorization_layer(train_ds,  max_tokens=None, \n",
    "                                  split='whitespace',\n",
    "                                  standardize=\"lower_and_strip_punctuation\",\n",
    "                                  output_mode=\"int\",\n",
    "                                  output_sequence_length=None,\n",
    "                                  ngrams=None, pad_to_max_tokens=False,\n",
    "                                  verbose=True,\n",
    "                                  **kwargs,\n",
    "                                 ):\n",
    "    # Build the text vectorization layer\n",
    "    text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "        max_tokens=max_tokens,\n",
    "        standardize=standardize, \n",
    "        output_mode=output_mode,\n",
    "        output_sequence_length=output_sequence_length,\n",
    "        **kwargs\n",
    "    )\n",
    "    # Get just the text from the training data\n",
    "    if isinstance(train_ds, (np.ndarray, list, tuple, pd.Series)):\n",
    "        ds_texts = train_ds\n",
    "    else:\n",
    "        try:\n",
    "            ds_texts = train_ds.map(lambda x, y: x )\n",
    "        except:\n",
    "            ds_texts = train_ds\n",
    "            \n",
    "    # Fit the layer on the training texts\n",
    "    text_vectorizer.adapt(ds_texts)\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        # Print the params\n",
    "        print( \"\\ntf.keras.layers.TextVectorization(\" )\n",
    "        config = text_vectorizer.get_config()\n",
    "        pprint(config,indent=4)\n",
    "        print(\")\")\n",
    "               \n",
    "    # SAVING VOCAB FOR LATER\n",
    "    # Getting list of vocab \n",
    "    vocab = text_vectorizer.get_vocabulary()\n",
    "    # Save dictionaries to look up words from ints \n",
    "    int_to_str  = {idx:word for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    return text_vectorizer, int_to_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# from sklearn.metrics import classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Our Custom Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test our module using the evaluate_classification function on the following model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'training data', 'test data'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "fpath_joblib = \"example-clf-model-with-data.joblib\"\n",
    "loaded = joblib.load(fpath_joblib)\n",
    "loaded.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;naivebayes&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;naivebayes&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
       "                ('naivebayes', MultinomialNB())])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model and data to evaluate\n",
    "model = loaded['model']\n",
    "X_train, y_train = loaded['training data']\n",
    "X_test, y_test = loaded['test data']\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Custom Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cust_fun'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import custom module\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcust_fun\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfn\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cust_fun'"
     ]
    }
   ],
   "source": [
    "# Import custom module\n",
    "import cust_fun as fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the Module to the Python Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/purvikansara/Documents/My Github/AML Code Along/Untitled/Lecture 1 Pt1 - Making a Custom Module',\n",
       " '/opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python310.zip',\n",
       " '/opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10',\n",
       " '/opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "# Check sys.path for python path\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to append the absolute filepath of the folder that contains our module.  (the folder up one level \"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/purvikansara/Documents/My Github/AML Code Along/Untitled'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the absolute file path of parent directory\n",
    "os.path.abspath('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/purvikansara/Documents/My Github/AML Code Along/Untitled'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add parent directory to python path\n",
    "sys.path.append( os.path.abspath('../'))\n",
    "sys.path[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try importing again\n",
    "import cust_fun as fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test fn.evaluate_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fn.evaluate_classification(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding to the Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now add the following function to the .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "# Writing a demo custom function for .py file\n",
    "def demo_function(name):\n",
    "    print(f'Hello, {name}!')\n",
    "\n",
    "demo_function('World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n",
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "fn.demo_function(\"World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autoreload to continue editing module while in-use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T21:34:47.018659Z",
     "iopub.status.busy": "2024-01-09T21:34:47.017865Z",
     "iopub.status.idle": "2024-01-09T21:34:47.028010Z",
     "shell.execute_reply": "2024-01-09T21:34:47.026490Z",
     "shell.execute_reply.started": "2024-01-09T21:34:47.018619Z"
    }
   },
   "source": [
    "Must run autoreload BEFORE importing the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import cust_fun as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "# Call custom function from imported file\n",
    "fn.demo_function('World')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Bonus) Making a Package with Sub-Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Official Packaging Tutorial](https://packaging.python.org/tutorials/packaging-projects/)\n",
    "- Our module is structure like this:\n",
    "```\n",
    "custom_package\n",
    "    ‚îî‚îÄ‚îÄ __init__.py\n",
    "    ‚îî‚îÄ‚îÄ evaluate.py\n",
    "    ‚îî‚îÄ‚îÄ nlp.py\n",
    "```\n",
    "\n",
    "- The folder can contain other py files as well, but its needs an `__init__.py` to be recognized as a package. \n",
    "- The other py files can be import inside of `__init__.py` to make them part of the namespace\\\n",
    "\n",
    "  \n",
    "```python\n",
    "# Contents of __init__.py:\n",
    "from . import evaluation\n",
    "from . import nlp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the autoreload extension\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import custom_package_SOLUTION  as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(cp.evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(cp.nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.evaluation.evaluate_classification(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Local Modules From Any Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **For using in different projects across your machine**, a slightly more advanced way to reuse your functions is to create a special directory (e.g., ~/my_python_modules) where you will store such .py files. Then add this directory to the PYTHONPATH environment variable:\n",
    "\n",
    "   - **On Unix/Linux/Mac**:\n",
    "     - Open your terminal.\n",
    "     - Open your shell's profile script (`.bashrc`, `.bash_profile`, or `.zshrc` for Bash/Zsh).\n",
    "     - Add this line to the end of the file: `export PYTHONPATH=\"${PYTHONPATH}:~/my_python_modules\"`\n",
    "     - Save and close the text editor.\n",
    "     - Either source the profile script (e.g., `source ~/.bashrc`) or close and re-open your terminal.\n",
    "   - **On Windows**:\n",
    "     - Open System Properties/Advanced/Environment Variables.\n",
    "     - Choose New under System variables.\n",
    "     - Enter PYTHONPATH as the variable name and the path to the directory as the variable value.\n",
    "     - Click OK until you're out of the dialog. The changes will take effect after a restart.\n",
    "\n",
    "   Now you can import your function in any Python script across your system like this:\n",
    "\n",
    "   ```python\n",
    "   import my_functions\n",
    "   \n",
    "   result = my_functions.add_two_numbers(1, 2)\n",
    "   print(result)  # Output: 3\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- ChatGPT Chat: https://chat.openai.com/share/d6d5daca-798a-405b-90de-8eb6e41435e1\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
